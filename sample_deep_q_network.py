# -*- coding: utf-8 -*-
"""Sample Deep Q Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SOYEbmhiREmaWk3b7Jw9Wl5stD-RBN8g

**Helper Functions**
"""

# GRIDBOARD.PY
import numpy as np
import random
import sys

def randPair(s, e):
  return np.random.randint(s,e), np.random.randint(s,e)

class BoardPiece:

  def __init__(self, name, code, pos):
    self.name = name # name of the piece
    self.code = code # an ASCII character to display on the board
    self.pos = pos # 2-tuple e.g. (1,4)

class BoardMask:

  def __init__(self, name, mask, code):
    self.name = name
    self.mask = mask
    self.code = code


  def get_positions(self):
    return np.nonzero(self.mask)

  
def zip_positions2d(positions):
  x, y = positions
  return list(zip(x,y))


class GridBoard:

  def __init__(self, size=4):
    self.size = size # Board dimensions, e.g. 4 * 4
    self.components = {} # name: board piece
    self.masks = {}

  def addPiece(self, name, code, pos=(0,0)):
    newPiece = BoardPiece(name, code, pos)
    self.components[name] = newPiece


  # Basically a set of boundary elements
  def addMask(self, name, mask, code):
    #mask is a 2D-np array with 1s where the boundary elements are
    newMask = BoardMask(name, mask, code)
    self.masks[name] = newMask

  def movePiece(self, name, pos):
    move = True
    for _, mask in self.masks.items():
      if pos in zip_positions2d(mask.get_positions()):
        move = False
    if move:
      self.components[name].pos = pos

  def delPiece(self, name):
    del self.components['name']

  def render(self):
    dtype = '<U2'
    displ_board = np.zeros((self.size, self.size), dtype=dtype)
    displ_board[:] = ' '
    
    for name, piece in self.components.items():
      displ_board[piece.pos] = piece.code

    for name, mask in self.masks.items():
      displ_board[mask.get_positions()] = mask.code

    return displ_board

  def render_np(self):
    num_pieces = len(self.components) + len(self.masks)
    displ_board = np.zeros((num_pieces, self.size, self.size), dtype=np.uint8)
    layer = 0

    for name, piece in self.components.items():
      pos = (layer,) + piece.pos
      displ_board[pos] = 1
      layer += 1

    for name, mask in self.masks.items():
      x,y = self.masks['boundary'].get_positions()
      z = np.repeat(layer, len(x))
      a = (z, x, y)
      displ_board[a] = 1
      layer += 1
    return displ_board

def addTuple(a,b):
  return tuple([sum(x) for x in zip(a,b)])

# GRIDWORLD.PY
from GridBoard import *

class Gridworld:

    def __init__(self, size=4, mode='static'):
        if size >= 4:
            self.board = GridBoard(size=size)
        else:
            print("Minimum board size is 4. Initialized to size 4.")
            self.board = GridBoard(size=4)

        #Add pieces, positions will be updated later
        self.board.addPiece('Player','P',(0,0))
        self.board.addPiece('Goal','+',(1,0))
        self.board.addPiece('Pit','-',(2,0))
        self.board.addPiece('Wall','W',(3,0))

        if mode == 'static':
            self.initGridStatic()
        elif mode == 'player':
            self.initGridPlayer()
        else:
            self.initGridRand()

    #Initialize stationary grid, all items are placed deterministically
    def initGridStatic(self):
        #Setup static pieces
        self.board.components['Player'].pos = (0,3) #Row, Column
        self.board.components['Goal'].pos = (0,0)
        self.board.components['Pit'].pos = (0,1)
        self.board.components['Wall'].pos = (1,1)

    #Check if board is initialized appropriately (no overlapping pieces)
    #also remove impossible-to-win boards
    def validateBoard(self):
        valid = True

        player = self.board.components['Player']
        goal = self.board.components['Goal']
        wall = self.board.components['Wall']
        pit = self.board.components['Pit']

        all_positions = [piece for name,piece in self.board.components.items()]
        all_positions = [player.pos, goal.pos, wall.pos, pit.pos]
        if len(all_positions) > len(set(all_positions)):
            return False

        corners = [(0,0),(0,self.board.size), (self.board.size,0), (self.board.size,self.board.size)]
        #if player is in corner, can it move? if goal is in corner, is it blocked?
        if player.pos in corners or goal.pos in corners:
            val_move_pl = [self.validateMove('Player', addpos) for addpos in [(0,1),(1,0),(-1,0),(0,-1)]]
            val_move_go = [self.validateMove('Goal', addpos) for addpos in [(0,1),(1,0),(-1,0),(0,-1)]]
            if 0 not in val_move_pl or 0 not in val_move_go:
                #print(self.display())
                #print("Invalid board. Re-initializing...")
                valid = False

        return valid

    #Initialize player in random location, but keep wall, goal and pit stationary
    def initGridPlayer(self):
        #height x width x depth (number of pieces)
        self.initGridStatic()
        #place player
        self.board.components['Player'].pos = randPair(0,self.board.size)

        if (not self.validateBoard()):
            #print('Invalid grid. Rebuilding..')
            self.initGridPlayer()

    #Initialize grid so that goal, pit, wall, player are all randomly placed
    def initGridRand(self):
        #height x width x depth (number of pieces)
        self.board.components['Player'].pos = randPair(0,self.board.size)
        self.board.components['Goal'].pos = randPair(0,self.board.size)
        self.board.components['Pit'].pos = randPair(0,self.board.size)
        self.board.components['Wall'].pos = randPair(0,self.board.size)

        if (not self.validateBoard()):
            #print('Invalid grid. Rebuilding..')
            self.initGridRand()

    def validateMove(self, piece, addpos=(0,0)):
        outcome = 0 #0 is valid, 1 invalid, 2 lost game
        pit = self.board.components['Pit'].pos
        wall = self.board.components['Wall'].pos
        new_pos = addTuple(self.board.components[piece].pos, addpos)
        if new_pos == wall:
            outcome = 1 #block move, player can't move to wall
        elif max(new_pos) > (self.board.size-1):    #if outside bounds of board
            outcome = 1
        elif min(new_pos) < 0: #if outside bounds
            outcome = 1
        elif new_pos == pit:
            outcome = 2

        return outcome

    def makeMove(self, action):
        #need to determine what object (if any) is in the new grid spot the player is moving to
        #actions in {u,d,l,r}
        def checkMove(addpos):
            if self.validateMove('Player', addpos) in [0,2]:
                new_pos = addTuple(self.board.components['Player'].pos, addpos)
                self.board.movePiece('Player', new_pos)

        if action == 'u': #up
            checkMove((-1,0))
        elif action == 'd': #down
            checkMove((1,0))
        elif action == 'l': #left
            checkMove((0,-1))
        elif action == 'r': #right
            checkMove((0,1))
        else:
            pass

    def reward(self):
        if (self.board.components['Player'].pos == self.board.components['Pit'].pos):
            return -10
        elif (self.board.components['Player'].pos == self.board.components['Goal'].pos):
            return 10
        else:
            return -1

    def display(self):
        return self.board.render()

"""# **LEARNING HOW TO PLAY GRIDWORLD**

**1. INTRODUCING GRIDWORLD GAME**
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload

from GridWorld import Gridworld
game = Gridworld(size=4, mode='static')

game.display()

game.makeMove('d')
game.makeMove('d')
game.makeMove('l')
game.display()

game.reward()

# actual representation of the game state
game.board.render_np()

game.board.render_np().shape

"""The state is a $4 * 4 * 4$ tensor where the first dimension indexes a set of four matrices of size $4 * 4$. Each matrix is a $4 * 4$ grid of zeros and a single 1, where 1 indicates the position of a particular object. Each matrix encodes the position of one of the four objects: the player, the goal, the pit and the wall. If we compare the result from *display* with the game state, we can see that the first matrix encode the position of the player, the second matrix encodes the position of the goal, the third matrix encodes the position of the pit, and the last matrix encodes the position of the wall.

**2. NEURAL NETWORK Q FUNCTION**
"""

import numpy as np
import torch
from GridWorld import Gridworld
import random
from matplotlib import pylab as plt

l1 = 64
l2 = 150
l3 = 100
l4 = 4

model = torch.nn.Sequential(
    torch.nn.Linear(l1, l2),
    torch.nn.ReLU(),
    torch.nn.Linear(l2, l3),
    torch.nn.ReLU(),
    torch.nn.Linear(l3, l4)
)

loss_fn = torch.nn.MSELoss()
learning_rate = 1e-3
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

gamma = 0.9
epsilon = 1.0

action_set = {
    0: 'u',
    1: 'd',
    2: 'l',
    3: 'r'
}

epochs = 1000
losses = []
# creates a list to store loss values so we can plot the trend later

for i in range(epochs):
  game = Gridworld(size=4, mode='static') 
  # for each epoch we start a new game

  state_ = game.board.render_np().reshape(1, 64) + np.random.rand(1, 64)/10.0
  # after game creation, we extract the state information and add a small 
  # amount of noise

  state1 = torch.from_numpy(state_).float()
  # converts the np.array into a torch tensor and torch variable

  status = 1
  # uses the status variable to keep tract of whether or not the game is still
  # in progress
  while (status == 1):
    qval = model(state1)
    # runs the Q-network forward to get its predicted Q values for all the 
    # actions
    qval_ = qval.data.numpy()
    if (random.random() < epsilon):
      # selects an action using the epsilon-greedy method
      action_ = np.random.randint(0, 4)
    else:
      action_ = np.argmax(qval_)

    action = action_set[action_]
    # translates the numerical action into one of the action characters
    # that our Gridworld game expects
    game.makeMove(action)
    # after selecting an action using the epsilon-greedy method, 
    # takes the action
    state2_ = game.board.render_np().reshape(1, 64) + np.random.rand(1,64)/10.0
    state2 = torch.from_numpy(state2_).float()
    # after making a move, gets the new state of the game
    reward = game.reward()
    with torch.no_grad():
      newQ = model(state2.reshape(1, 64))
    maxQ = torch.max(newQ)
    # finds the maximum Q value predicted from the new state
    if reward == -1:
      # calculates the target Q value
      Y = reward + (gamma * maxQ)
    else:
      Y = reward
    Y = torch.Tensor([Y]).detach()
    X = qval.squeeze()[action_]
    # creates a copy of the qval array and then updates the one element
    # corresponding to the action taken
    loss = loss_fn(X, Y)
    optimizer.zero_grad()
    loss.backward()
    losses.append(loss.item())
    optimizer.step()
    state1 = state2
    if reward != -1:
      # if reward is -1, the game hasn't been won or lost
      status = 0

# decrements the epsilon value each epoch
  if epsilon > 0.1:
    epsilon -= (1 / epochs)

plt.figure(figsize=(10,7))
plt.plot(losses)
plt.xlabel('Epochs', fontsize=22)
plt.ylabel('Loss', fontsize=22)

"""Example of a linear model with no_grad"""

m = torch.Tensor([2.0])
m.requires_grad=True
b = torch.Tensor([1.0])
b.requires_grad=True
def linear_model(x, m, b):
  y = m @ x + b
  return y

y = linear_model(torch.Tensor([4.]), m, b)
y
y.grad_fn
with torch.no_grad():
  y = linear_model(torch.Tensor([4.0]), m,b)
y
y.grad_fn

"""**3. TESTING THE MODEL**"""

def test_model(model, mode='static', display=True):
  i = 0
  test_game = Gridworld(mode=mode)
  state_ = test_game.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0
  state = torch.from_numpy(state_).float()
  if display:
    print('Initial State:')
    print(test_game.display())
  status = 1
  while (status == 1):
    qval = model(state)
    qval_ = qval.data.numpy()
    action_ = np.argmax(qval_)
    action = action_set[action_]
    if display:
      print('Move #: %s; Taking action: %s' % (i, action))
    test_game.makeMove(action)
    state_ = test_game.board.render_np().reshape(1,64)+np.random.rand(1,64)/10.0
    state = torch.from_numpy(state_).float()
    if display:
      print(test_game.display())
    reward = test_game.reward()
    if reward != -1:
      if reward > 0:
        status = 2
        if display:
          print('Game won! Reward: %s' % (reward,))
      else:
        status = 0
        if display:
          print('Game LOST. Reward: %s' % (reward,))
    i += 1
    if (i > 15):
      if display:
        print('Game LOST. Too many moves.')
      break

  win = True if status == 2 else False
  return win

test_model(model, 'static')

test_model(model, 'random')

"""# **DQN with Experience Replay**"""

l1 = 64
l2 = 150
l3 = 100
l4 = 4

model = torch.nn.Sequential(
    torch.nn.Linear(l1, l2),
    torch.nn.ReLU(),
    torch.nn.Linear(l2, l3),
    torch.nn.ReLU(),
    torch.nn.Linear(l3, l4)
)
loss_fn = torch.nn.MSELoss()
learning_rate = 1e-3
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

gamma = 0.9
epsilon = 0.3

from collections import deque
from IPython.display import clear_output
epochs = 5000
losses = []
mem_size = 1000 #A
batch_size = 200 #B
replay = deque(maxlen=mem_size) #C
max_moves = 50 #D
h = 0

for i in range(epochs):
  game = Gridworld(size = 4, mode='random')
  state1_ = game.board.render_np().reshape(1, 64) + np.random.rand(1,64)/100.0
  state1 = torch.from_numpy(state1_).float()
  status = 1
  mov = 0
  while (status==1):
    mov += 1
    qval = model(state1) #E
    qval_ = qval.data.numpy()
    if (random.random() < epsilon):
      #F
      action_ = np.random.randint(0, 4)
    else:
      action_ = np.argmax(qval_)

    action = action_set[action_]
    game.makeMove(action)
    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0
    state2 = torch.from_numpy(state2_).float()
    reward = game.reward()
    done = True if reward > 0 else False
    exp = (state1, action_, reward, state2, done) #G
    replay.append(exp)
    state1 = state2

    if len(replay) > batch_size: 
      #I
      minibatch = random.sample(replay, batch_size)
      state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in minibatch]) #K
      action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])
      reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])
      state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])
      done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])

      Q1 = model(state1_batch) #L
      with torch.no_grad():
        Q2 = model(state2_batch)

      Y = reward_batch + gamma * ((1 - done_batch) * torch.max(Q2, dim=1)[0]) #N
      X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()
      loss = loss_fn(X, Y.detach())
      print(i, loss.item())
      clear_output(wait=True)
      optimizer.zero_grad()
      loss.backward()
      losses.append(loss.item())
      optimizer.step()

    if reward != -1 or mov > max_moves:
      #O
      status = 0
      mov = 0

losses = np.array(losses)

#A Set the total size of the experience replay memory
#B Set the minibatch size
#C Create the memory replay as a deque list
#D Maximum number of moves before game is over
#E Compute Q-values from input state in order to select action
#F Select action using epsilon-greedy strategy
#G Create experience of state, reward, action and next state as a tuple
#H Add experience to experience replay list
#I If replay list is at least as long as minibatch size, begin minibatch training
#J Randomly sample a subset of the replay list
#K Separate out the components of each experience into separate minibatch tensors
#L Re-compute Q-values for minibatch of states to get gradients
#M Compute Q-values for minibatch of next states but don't compute gradients
#N Compute the target Q-values we want the DQN to learn
#O If game is over, reset status and mov number

"""Testing the performance with experience replay"""

plt.figure(figsize=(10,7))
plt.plot(losses)

max_games = 1000
wins = 0
for i in range(max_games):
  win = test_model(model, mode='random', display=False)
  if win:
    wins += 1
win_perc = float(wins) / float(max_games)
print('Games played: {0}, # of wins: {1}'.format(max_games, wins))
print('Win percentage: {}%'.format(100.0*win_perc))

"""# **DQN with experience replay and target network**"""

import copy

l1 = 64
l2 = 150
l3 = 100
l4 = 4

model = torch.nn.Sequential(
    torch.nn.Linear(l1,l2),
    torch.nn.ReLU(),
    torch.nn.Linear(l2, l3),
    torch.nn.ReLU(),
    torch.nn.Linear(l3, l4)
)

model2 = copy.deepcopy(model) #A
model2.load_state_dict(model.state_dict()) #B

loss_fn = torch.nn.MSELoss()
learning_rate = 1e-3
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

gamma = 0.9
epsilon = 0.3

from collections import deque
from IPython.display import clear_output
epochs = 5000
losses = []
mem_size = 1000
batch_size = 200
replay = deque(maxlen=mem_size)
max_moves = 50
h = 0
sync_freq = 500 #A
j = 0
for i in range(epochs):
  game = Gridworld(size=4, mode='random')
  state1_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0
  state1 = torch.from_numpy(state1_).float()
  status = 1
  mov = 0
  while (status == 1):
    j += 1
    mov += 1
    qval = model(state1)
    qval_ = qval.data.numpy()
    if (random.random() < epsilon):
      action_ = np.random.randint(0,4)
    else:
      action_ = np.argmax(qval_)

    action = action_set[action_]
    game.makeMove(action)
    state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0
    state2 = torch.from_numpy(state2_).float()
    reward = game.reward()
    done = True if reward > 0 else False
    exp = (state1, action_, reward, state2, done)
    replay.append(exp) #A
    state1 = state2

    if len(replay) > batch_size:
      minibatch = random.sample(replay, batch_size)
      state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in minibatch])
      action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])
      reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])
      state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])
      done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])
      Q1 = model(state1_batch) 
      with torch.no_grad():
          Q2 = model2(state2_batch) #B
            
      Y = reward_batch + gamma * ((1-done_batch) * torch.max(Q2,dim=1)[0])
      X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()
      loss = loss_fn(X, Y.detach())
      print(i, loss.item())
      clear_output(wait=True)
      optimizer.zero_grad()
      loss.backward()
      losses.append(loss.item())
      optimizer.step()
            
      if j % sync_freq == 0: #C
          model2.load_state_dict(model.state_dict())
    if reward != -1 or mov > max_moves:
        status = 0
        mov = 0
        
losses = np.array(losses)

#A Set the update frequency for synchronizing the target model parameters to the main DQN
#B Use the target network to get the maiximum Q-value for the next state
#C Copy the main model parameters to the target network

plt.figure(figsize=(10,7))
plt.plot(losses)

max_games = 10000
wins = 0
for i in range(max_games):
  win = test_model(model, mode='random', display=False)
  if win:
    wins += 1
win_perc = float(wins) / float(max_games)
print('Games played: {0}, # of wins: {1}'.format(max_games, wins))
print('Win percentage: {}%'.format(100.0*win_perc))

test_model(model, 'random')