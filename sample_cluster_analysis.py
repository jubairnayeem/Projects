# -*- coding: utf-8 -*-
"""Sample Cluster Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15107fxk30jZ2P_tavyKpYFL_krnyYOnG

# **k-means clustering**
"""

from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=150, n_features=2, 
                  centers=3, cluster_std=0.5,
                  shuffle=True, random_state=0)
import matplotlib.pyplot as plt
plt.scatter(X[:, 0], X[:, 1], c='white', 
            marker='o', edgecolor='black',
            s=50)
plt.grid()
plt.tight_layout()
plt.show()

from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, init='random',
            n_init=10, max_iter=300, 
            tol=1e-04, random_state=0)
y_km = km.fit_predict(X)

"""1. n_clusters = number of cluster
2. n_init = number of iteration with different initialization
"""

plt.scatter(X[y_km == 0, 0],
            X[y_km == 0, 1],
            s=50, c='lightgreen',
            marker='s', edgecolor='black',
            label='Cluster 1')
plt.scatter(X[y_km == 1, 0],
            X[y_km == 1, 1],
            s=50, c='orange',
            marker='o', edgecolor='black',
            label='Cluster 2')
plt.scatter(X[y_km == 2, 0],
            X[y_km == 2, 1],
            s=50, c='lightblue',
            marker='v', edgecolor='black',
            label='Cluster 3')
plt.scatter(km.cluster_centers_[:, 0],
            km.cluster_centers_[:, 1],
            s=250, c='red',
            marker='*', edgecolor='black',
            label='Centroids')
plt.legend(loc='best')
plt.grid()
plt.tight_layout()
plt.show()

"""### **K-means++**

1. Initialize an empty set, M, to store the k centroids being selected.
2. Randomly choose the first centroid $µ^j$ from the input examples and assign it to M
3. For each example, $x^i$ that is not in M, find the minimum squared distance, $d(x^i, M)^2$ to any centroids in M
4. To randomly select the next centroid, $µ^p$ use a weighted probability distribution equal to $d(µ^p, M)^2 / sum_id(x^i, M)^2$
5. Repear steps 2 and 3 until k centroids are chosen
6. Proceed with the classic k-means

*Replace the init parameter of kmeans to 'k-means++'*

### **Elbow method to find the optimal number of clusters**
Elbow method can be used to estimate the optimal number of cluster, k, for a given task. If k increases the distrotion will decrease.
"""

print('Distortion: %.2f' % km.inertia_)

distortions = []
for i in range(1, 11):
  km = KMeans(n_clusters=i, init='k-means++',
              n_init=10, max_iter=300, random_state=0)
  km.fit(X)
  distortions.append(km.inertia_)
plt.plot(range(1, 11), distortions, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.tight_layout()
plt.show()

"""# Quantifying the quality of clustering via silhouette plots

Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the examples in the clusters are. The silhouette coefficient is bounded in the range -1 to 1. An ideal of 1 is if $b^i >> a^i$, since $b^i$ quantifies how dissimilar an example is from other clusters, and $a^i$ tells us how similar it is to the other examples in its own cluster. 
"""

km = KMeans(n_clusters=3, init='k-means++',
            n_init=10, max_iter=300, tol=1e-04, 
            random_state=0)
y_km = km.fit_predict(X)
import numpy as np
from matplotlib import cm
from sklearn.metrics import silhouette_samples
cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[0]
silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')
y_ax_lower, y_ax_upper = 0, 0 
yticks = []
for i, c in enumerate(cluster_labels):
  c_silhouette_vals = silhouette_vals[y_km == c]
  c_silhouette_vals.sort()
  y_ax_upper += len(c_silhouette_vals)
  color = cm.jet(float(i) / n_clusters)
  plt.barh(range(y_ax_lower, y_ax_upper),
           c_silhouette_vals, 
           height=1.0,
           edgecolor='none',
           color=color)
  yticks.append((y_ax_lower + y_ax_upper) / 2.0)
  y_ax_lower += len(c_silhouette_vals)

silhouette_avg = np.mean(silhoutte_vals)
plt.axvline(silhouette_avg, color='red', linestyle='--')
plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Cluster')
plt.xlabel('Silhouette coefficient')
plt.tight_layout()
plt.show()

"""From the silhouette plot, we can see that the silhouette coefficients are not even close to 0 which is an indicator of a good clustering. 

# **Organizing clusters as a hierarchical tree**

Hierarchical complete linkage clustering is an iterative procedure that can be summarized by the following steps:
1. Compute the distance matrix of all examples
2. Represent each data point as a singleton cluster
3. Merge the two closest clusters based on the distance between the most dissimilar members
4. Update the similarity matrix
5. Repeat steps 2-4 until one single cluster remains
"""

# Generating the random data sample to work
import pandas as pd
import numpy as np
np.random.seed(123)
variables = ['X', 'Y', 'Z']
labels = ['ID_0', 'ID_1', 'ID_2', 'ID_3', 'ID_4']
X = np.random.random_sample([5, 3]) * 10
df = pd.DataFrame(X, columns=variables, index=labels)
df

# Computing the distance matrix
from scipy.spatial.distance import pdist, squareform
row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')), 
                        columns=labels, index=labels)
row_dist

# Linkage matrix
from scipy.cluster.hierarchy import linkage
row_clusters = linkage(pdist(df, metric='euclidean'),
                      method='complete')
pd.DataFrame(row_clusters, columns=['row label 1', 'row label 2', 'distance',
                                    'no. of items in clust.'], 
             index=['cluster %d' % (i+1) for i in range(row_clusters.shape[0])])

from scipy.cluster.hierarchy import dendrogram
row_dendr = dendrogram(row_clusters, labels=labels)
plt.tight_layout()
plt.ylabel('Euclidean Distance')
plt.show()

"""**Attaching dendrograms to a heat map**"""

# 1) Creating a new figure object & define the x axis position, y axis position,
# width, and height of the dendrogram. We also rotate the dendrogram 90degrees
# counter-clockwise
fig = plt.figure(figsize=(8,8), facecolor='white') 
axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])
row_dendr = dendrogram(row_clusters, orientation='left')

# 2) Reordering the data in our initial df according to the clustering labels
# that can be accessed from the dendrogram object
df_rowclust = df.iloc[row_dendr['leaves'][::-1]]

# 3) Construct the heat map from the reordered DF and position it next to the 
# dendrogram
axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])
cax = axm.matshow(df_rowclust, interpolation='nearest', cmap='hot_r')

# 4) Modifying the aesthetics
axd.set_xticks([])
axd.set_yticks([])
for i in axd.spines.values():
  i.set_visible(False)

fig.colorbar(cax)
axm.set_xticklabels([''] + list(df_rowclust.columns))
axm.set_yticklabels([''] + list(df_rowclust.index))
plt.show()

"""**Applying agglomerative clustering using sklearn**"""

from sklearn.cluster import AgglomerativeClustering
ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean',
                             linkage='complete')
labels = ac.fit_predict(X)
print("Cluster Labels: %s" %labels)

"""# **DBSCAN**
Density-based clustering assigns cluster labels based on dense regions of points. 
"""

from sklearn.datasets import make_moons
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)
plt.scatter(X[:, 0], X[:, 1])
plt.tight_layout()
plt.show()

from sklearn.cluster import DBSCAN
db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean')
y_db = db.fit_predict(X)
plt.scatter(X[y_db == 0, 0],
            X[y_db == 0, 1],
            c='lightblue',
            edgecolor='black',
            marker='o',
            s=40,
            label='Cluster 1')
plt.scatter(X[y_db == 1, 0],
            X[y_db == 1, 1],
            c='red', 
            edgecolor='black',
            s=40, 
            label='Cluster 2')
plt.legend()
plt.tight_layout()
plt.show()