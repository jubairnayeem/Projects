# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11LvYXJiFeGXgSQNtnG2Z-wUekwuDeFky

# Preparing the IMDb movie review data for text processing
"""

# Getting the data
# sentiment data = https://ai.stanford.edu/~amaas/data/sentiment/
import tarfile
with tarfile.open('/content/aclImdb_v1.tar.gz', 'r:gz') as tar:
  tar.extractall()

pip install pyprind

# Preprocessing the movie dataset into a more convenient format
import pyprind
import pandas as pd
import os

# change the 'basepath' to the directory of the unzipped movie dataset
basepath = '/content/aclImdb'
labels = {'pos': 1, 'neg': 0}
pbar = pyprind.ProgBar(50000)
df = pd.DataFrame()
for s in ('test', 'train'):
  for l in ('pos', 'neg'):
    path = os.path.join(basepath, s, l)
    for file in sorted(os.listdir(path)):
      with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:
        txt = infile.read()
      df = df.append([[txt, labels[l]]], ignore_index = True)
      pbar.update()

df.columns = ['review', 'sentiment']

# Storing the movie review dataset as a CSV file
import numpy as np
np.random.seed(0)
df = df.reindex(np.random.permutation(df.index))
df.to_csv('movie_data.csv', index=False, encoding='utf-8')

df = pd.read_csv('movie_data.csv', encoding='utf-8')
df.head(3)
#df.shape

"""## Bag-of-words model
The bag-of-words model allows us to represent text as numerical feature vectors. The main idea can be summarized below:
1. Create a vocabulary of unique tokens - for example, words - from the entire set of documents
2. Construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.

We can construct a bag-of-words model based on the word counts in the respective documents using the CountVectorizer class from sklearn.
"""

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()
docs = np.array(['The sun is shining',
                 'The weather is sweet',
                 'The sun is shining, the weather is sweet, and one and one is two'])
bag = count.fit_transform(docs)

print(count.vocabulary_)

"""The vocabulary is stored in a Python dictionary that maps the unique words to integer indices. """

print(bag.toarray())

"""Each index position in the feature vectors corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the first feature at index position 0 resembles the count of word 'and' etc. 

## N-gram models
"""

count2 = CountVectorizer(ngram_range=(2,2))
docs2 = np.array(['The sun is shining',
                 'The weather is sweet',
                 'The sun is shining, the weather is sweet, and one and one is two'])
bag2 = count2.fit_transform(docs2)

print(count2.vocabulary_)

print(bag2.toarray())

"""## Assessing word relevancy via term frequency-inverse document frequency
A useful technique called the ***term frequency-inverse document frequency(tf-idf)*** can be used to downweight some frequently occuring words in the feature vector which are irrelevant to the analysis
"""

from sklearn.feature_extraction.text import TfidfTransformer
tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)
np.set_printoptions(precision=2)
print(tfidf.fit_transform(count.fit_transform(docs)).toarray())

"""As we can see the word 'is' had the largest term frequency in the third document, being the most frequently occuring word. However, after transforming the same featutre vector into tf-idfs, the word 'is' is now associated with a relatively small tf-idf (0.45) since it is also present in the first and second document. 

## Cleaning text data
"""

df.loc[0, 'review'][-50:]
# As we can see the text contains HTML markup as well as punctuation 
# We will clean the data as follows

import re
def preprocessor(text):
  text = re.sub('<[^>]*>', '', text)
  emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)
  text = (re.sub('[\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')) 
  return text

preprocessor(df.loc[0, 'review'][-50:])

preprocessor('</a>This :) is :( a test :-)!')

# Applying the preprocessor function to the overall dataset
df['review'] = df['review'].apply(preprocessor)

"""## Processing documents into tokens"""

# One way to tokenize documents is to split them into individual words by 
# splitting the cleaned documents at their whitespace characters
def tokenizer(text):
  return text.split()
tokenizer('runners like running and thus they run')

# Another useful technique is word stemming, which is the process of 
# transforming a word into its root form. It allows us to map related
# words to the same stem
from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()
def tokenizer_porter(text):
  return [porter.stem(word) for word in text.split()]

tokenizer_porter('runners like running and thus they run')

"""#### stop-word removal
stop-word are those words that are extremely common in all sorts of texts and probably bear no useful information such as *is, and, has, like*
"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')
[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]

"""## Training a logistic regression model for document classification"""

# Dividing the dataframe of cleaned text documents into 25000 documents 
# for training and 25000 documents for testing
X_train = df.loc[:25000, 'review'].values
y_train = df.loc[:25000, 'sentiment'].values
X_test = df.loc[25000:, 'review'].values
y_test = df.loc[25000:, 'sentiment'].values

# Using the GridSearchCV to find the optimal set of parameters for our 
# logistic regression model using 5-fold stratified CV
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(strip_accents=None, lowercase=False,
                        preprocessor=None)
param_grid = [{'vect__ngram_range': [(1,1)],
               'vect__stop_words': [stop, None],
               'vect__tokenizer': [tokenizer, tokenizer_porter],
               'clf__penalty': ['l1', 'l2'],
               'clf__C': [1.0, 10.0, 100.0]},
              {'vect__ngram_range': [(1,1)],
               'vect__stop_words': [stop, None],
               'vect__tokenizer': [tokenizer, tokenizer_porter],
               'vect__use_idf': [False],
               'vect__norm': [None],
               'clf__penalty': ['l1', 'l2'],
               'clf__C': [1.0, 10.0, 100.0]}]
lr_tfidf = Pipeline([('vect', tfidf), 
                     ('clf', 
                      LogisticRegression(random_state=0, solver='liblinear'))])
gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5,
                           verbose=2, n_jobs=-1)
gs_lr_tfidf.fit(X_train, y_train)

"""Our param_grid consisted of two parameters dictionaries. In the first dictionary, we used TfidVectorizer with its default settings and in the second dictionary we set those parameters to train a model based on raw term frequencies."""

# printing the best parameter set
print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)

"""As we can see, the best grid results using the regular *tokenizer* without Porter stemming, no stop-word library and tfidfs in combination with a logistic regression classifier that uses L2 regularization with the regularization strength of C of 10.0."""

# printing the average 5-fold cross validation accuracy scores on the training 
# dataset and the classification accuracy on the test dataset
print('CV accuracy: %.3f' % gs_lr_tfidf.best_score_)
clf = gs_lr_tfidf.best_estimator_
print('Test Accuracy: %.3f' % clf.score(X_test, y_test))
# Our model can predict whether a movie review is positive/negative with 90%
# accuracy

"""# Working with bigger data - online algorithms and out of core learning

* Out of core learning allows us to work with large datasets by fitting the classifier incrementally on smaller batches of a dataset. 
"""

import numpy as np
import re
from nltk.corpus import stopwords
stop = stopwords.words('english')
# defining a tokenizer function that cleans the unprocessed text data from 
# the movie_data.csv file and separating it into word tokens while removing
# stop-words
def tokenizer(text):
  text = re.sub('<[^>]*>', '', text)
  emoticons = re.findall('(?::|;|=)?(?:\)|\(|D|P)', text.lower())
  text = re.sub('[\W]+', ' ', text.lower() + ' '.join(emoticons).replace('-', ''))
  tokenized = [w for w in text.split() if w not in stop]
  return tokenized

# defining a generator function that reads in and returns one document at a time
def stream_docs(path):
  with open(path, 'r', encoding='utf-8') as csv:
    next(csv) # skip header
    for line in csv:
      text, label = line[:-3], int(line[-2])
      yield text, label

# checking the function by reading in the first document from the data file
next(stream_docs(path='movie_data.csv'))

# defining a function that will take a document stream from the stream_docs
# function and return a particular number of documents specified by the size
# parameter
def get_minibatch(doc_stream, size):
  docs, y = [], []
  try:
    for _ in range(size):
      text, label = next(doc_stream)
      docs.append(text)
      y.append(label)
  except StopIteration:
    return None, None
  return docs, y

# Here we will use the HashingVectorizer 
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier
vect = HashingVectorizer(decode_error='ignore', n_features=2**21,
                         preprocessor=None, tokenizer=tokenizer)
clf = SGDClassifier(loss='log', random_state=1)
doc_stream = stream_docs(path='movie_data.csv')

# starting the out-of-core learning
import pyprind
pbar = pyprind.ProgBar(45)
classes = np.array([0, 1])
for _ in range(45):
  X_train, y_train = get_minibatch(doc_stream, size=1000)
  if not X_train:
    break
  X_train = vect.transform(X_train)
  clf.partial_fit(X_train, y_train, classes=classes)
  pbar.update()

# We iterated over 45 mini-batches of documents where each mini-batch consists 
# of 1000 documents. We will now use the last 5000 documents to evaluate the 
# performance of our model
X_test, y_test = get_minibatch(doc_stream, size=5000)
X_test = vect.transform(X_test)
print("Accuracy: %.3f" %clf.score(X_test, y_test))

# Finally we can use the last 5000 documents to update our model
clf = clf.partial_fit(X_test, y_test)
clf

"""# Topic modeling with Latent Dirichlet Allocation (LDA)

LDA is a generative probabilistic model that tries to find groups of word that appear frequently together across different documents. The input to an LDA is the bag-of-words model. LDA decomposes it into two new matrices: 
1. A document-to-topic matrix
2. A word-to-topic matrix

We will categorize to 10 different topics on the movie dataset
"""

import pandas as pd
df = pd.read_csv('movie_data.csv', encoding='utf-8')

# creating a bag of words matrix
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer(stop_words='english',
                        max_df=.1, # maximum document frequency 10%
                        max_features=5000) # most frquently occuring words)
X = count.fit_transform(df['review'].values)
# Fitting the LDA model
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=10, random_state=123,
                                learning_method='batch')
X_topics = lda.fit_transform(X)

# Accessing to the components_ attribute of the lda instance that stores a 
# matrix containing the word importance (here 5000) for each of the 10 topics
# in increasing order
lda.components_.shape

# Analyzing the result by printing the five most important words for each of 
# the 10 topics. 
n_top_words = 5
feature_names = count.get_feature_names()
for topic_idx, topic in enumerate(lda.components_):
  print('Topic %d:' %(topic_idx + 1))
  print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))

"""Based on reading the five most important words for each topic, the LDA identified the following topics
1. Bad movies
2. Family movies
3. War movies
4. Art movies
5. Crime movies
6. Horror movies
7. Comedy movies
8. TV shows
9. Books movies
10. Action movies
"""

# plotting three movies from the horror movie category
horror = X_topics[:, 5].argsort()[::-1]
for iter_idx, movie_idx in enumerate(horror[:3]):
  print('\nHorror movie #%d:' % (iter_idx + 1))
  print(df['review'][movie_idx][:300], '...')

